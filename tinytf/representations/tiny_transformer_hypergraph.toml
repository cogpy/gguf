# TOML Hypergraph Representation of TinyTransformer
# Generated by gguf-workbench
# Format: Explicit hypergraph with vertices and hyperedges

[metadata]
model_name = "TinyTransformer"
architecture = "transformer"
representation = "hypergraph"
vocab_size = 10
embedding_dim = 5
context_length = 5
num_blocks = 1
num_heads = 1
feedforward_dim = 5
total_parameters = 200
format_version = "1.0"

# === Vertices ===
# Format: [vertices.<id>]

[vertices.input_tokens]
type = "tensor"
dtype = "int64"
shape = ['batch', 5]
[vertices.input_tokens.properties]
description = "Input token IDs"
range = [0, 9]

[vertices.embedding_weights]
type = "parameter"
dtype = "float32"
shape = [10, 5]
[vertices.embedding_weights.properties]
description = "Token embedding matrix"
parameter_count = 50

[vertices.embeddings]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.embeddings.properties]
description = "Token embeddings"

[vertices.position_embeddings]
type = "tensor"
dtype = "float32"
shape = [5, 5]
[vertices.position_embeddings.properties]
description = "Position embeddings"

[vertices.query_weights]
type = "parameter"
dtype = "float32"
shape = [5, 5]
[vertices.query_weights.properties]
description = "Query projection weights"
parameter_count = 25

[vertices.key_weights]
type = "parameter"
dtype = "float32"
shape = [5, 5]
[vertices.key_weights.properties]
description = "Key projection weights"
parameter_count = 25

[vertices.value_weights]
type = "parameter"
dtype = "float32"
shape = [5, 5]
[vertices.value_weights.properties]
description = "Value projection weights"
parameter_count = 25

[vertices.queries]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.queries.properties]
description = "Query vectors"

[vertices.keys]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.keys.properties]
description = "Key vectors"

[vertices.values]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.values.properties]
description = "Value vectors"

[vertices.attention_scores]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.attention_scores.properties]
description = "Attention score matrix"

[vertices.attention_output]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.attention_output.properties]
description = "Attention output"

[vertices.ffn_w1]
type = "parameter"
dtype = "float32"
shape = [5, 5]
[vertices.ffn_w1.properties]
description = "FFN first layer weights"
parameter_count = 25

[vertices.ffn_w2]
type = "parameter"
dtype = "float32"
shape = [5, 5]
[vertices.ffn_w2.properties]
description = "FFN second layer weights"
parameter_count = 25

[vertices.ffn_hidden]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.ffn_hidden.properties]
description = "FFN hidden activations"

[vertices.ffn_output]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 5]
[vertices.ffn_output.properties]
description = "FFN output"

[vertices.output_weights]
type = "parameter"
dtype = "float32"
shape = [5, 10]
[vertices.output_weights.properties]
description = "Output projection weights"
parameter_count = 50

[vertices.logits]
type = "tensor"
dtype = "float32"
shape = ['batch', 5, 10]
[vertices.logits.properties]
description = "Output logits"

# === Hyperedges ===
# Format: [hyperedges.<id>]
# Each hyperedge has sources (inputs) and targets (outputs)

[hyperedges.embed]
operation = "embedding_lookup"
sources = ['input_tokens', 'embedding_weights']
targets = ['embeddings']
[hyperedges.embed.properties]
description = "Token embedding lookup"

[hyperedges.project_queries]
operation = "matmul"
sources = ['embeddings', 'query_weights']
targets = ['queries']
[hyperedges.project_queries.properties]
description = "Project to query space"

[hyperedges.project_keys]
operation = "matmul"
sources = ['embeddings', 'key_weights']
targets = ['keys']
[hyperedges.project_keys.properties]
description = "Project to key space"

[hyperedges.project_values]
operation = "matmul"
sources = ['embeddings', 'value_weights']
targets = ['values']
[hyperedges.project_values.properties]
description = "Project to value space"

[hyperedges.compute_attention_scores]
operation = "scaled_dot_product"
sources = ['queries', 'keys']
targets = ['attention_scores']
[hyperedges.compute_attention_scores.properties]
description = "Compute attention scores"
scaling_factor = 0.447

[hyperedges.apply_attention]
operation = "attention_aggregate"
sources = ['attention_scores', 'values']
targets = ['attention_output']
[hyperedges.apply_attention.properties]
description = "Apply attention to values"

[hyperedges.ffn_layer1]
operation = "matmul_activation"
sources = ['attention_output', 'ffn_w1']
targets = ['ffn_hidden']
[hyperedges.ffn_layer1.properties]
description = "FFN first layer"
activation = "relu"

[hyperedges.ffn_layer2]
operation = "matmul"
sources = ['ffn_hidden', 'ffn_w2']
targets = ['ffn_output']
[hyperedges.ffn_layer2.properties]
description = "FFN second layer"

[hyperedges.output_projection]
operation = "matmul"
sources = ['ffn_output', 'output_weights']
targets = ['logits']
[hyperedges.output_projection.properties]
description = "Project to vocabulary"

# === Weights ===
# Format: [weights.<param_id>]

[weights.embedding_weights]
# Shape: [10, 5]
values = [
  [0.4967, -0.1383, 0.6477, 1.523, -0.2342],
  [-0.2341, 1.5792, 0.7674, -0.4695, 0.5426],
  [-0.4634, -0.4657, 0.242, -1.9133, -1.7249],
  [-0.5623, -1.0128, 0.3142, -0.908, -1.4123],
  [1.4656, -0.2258, 0.0675, -1.4247, -0.5444],
  [0.1109, -1.151, 0.3757, -0.6006, -0.2917],
  [-0.6017, 1.8523, -0.0135, -1.0577, 0.8225],
  [-1.2208, 0.2089, -1.9597, -1.3282, 0.1969],
  [0.7385, 0.1714, -0.1156, -0.3011, -1.4785],
  [-0.7198, -0.4606, 1.0571, 0.3436, -1.763],
]

[weights.query_weights]
# Shape: [5, 5]
values = [
  [0.3241, -0.3851, -0.6769, 0.6117, 1.031],
  [0.9313, -0.8392, -0.3092, 0.3313, 0.9755],
  [-0.4792, -0.1857, -1.1063, -1.1962, 0.8125],
  [1.3562, -0.072, 1.0035, 0.3616, -0.6451],
  [0.3614, 1.538, -0.0358, 1.5646, -2.6197],
]

[weights.key_weights]
# Shape: [5, 5]
values = [
  [0.8219, 0.087, -0.299, 0.0918, -1.9876],
  [-0.2197, 0.3571, 1.4779, -0.5183, -0.8085],
  [-0.5018, 0.9154, 0.3288, -0.5298, 0.5133],
  [0.0971, 0.9686, -0.7021, -0.3277, -0.3921],
  [-1.4635, 0.2961, 0.2611, 0.0051, -0.2346],
]

[weights.value_weights]
# Shape: [5, 5]
values = [
  [-1.4154, -0.4206, -0.3427, -0.8023, -0.1613],
  [0.4041, 1.8862, 0.1746, 0.2576, -0.0744],
  [-1.9188, -0.0265, 0.0602, 2.4632, -0.1924],
  [0.3015, -0.0347, -1.1687, 1.1428, 0.7519],
  [0.791, -0.9094, 1.4028, -1.4019, 0.5869],
]

[weights.ffn_w1]
# Shape: [5, 5]
values = [
  [2.1905, -0.9905, -0.5663, 0.0997, -0.5035],
  [-1.5507, 0.0686, -1.0623, 0.4736, -0.9194],
  [1.5499, -0.7833, -0.3221, 0.8135, -1.2309],
  [0.2275, 1.3071, -1.6075, 0.1846, 0.2599],
  [0.7818, -1.237, -1.3205, 0.5219, 0.297],
]

[weights.ffn_w2]
# Shape: [5, 5]
values = [
  [0.2505, 0.3464, -0.68, 0.2323, 0.2931],
  [-0.7144, 1.8658, 0.4738, -1.1913, 0.6566],
  [-0.9747, 0.7871, 1.1586, -0.8207, 0.9634],
  [0.4128, 0.8221, 1.8968, -0.2454, -0.7537],
  [-0.8895, -0.8158, -0.0771, 0.3412, 0.2767],
]

[weights.output_weights]
# Shape: [5, 10]
values = [
  [0.8272, 0.013, 1.4535, -0.2647, 2.7202, 0.6257, -0.8572, -1.0709, 0.4825, -0.2235],
  [0.714, 0.4732, -0.0728, -0.8468, -1.5148, -0.4465, 0.8564, 0.2141, -1.2457, 0.1732],
  [0.3853, -0.8839, 0.1537, 0.0582, -1.143, 0.3578, 0.5608, 1.0831, 1.0538, -1.3777],
  [-0.9378, 0.515, 0.5138, 0.515, 3.8527, 0.5709, 1.1356, 0.954, 0.6514, -0.3153],
  [0.759, -0.7728, -0.2368, -0.4854, 0.0819, 2.3147, -1.8673, 0.6863, -1.6127, -0.4719],
]
