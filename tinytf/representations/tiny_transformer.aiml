<?xml version="1.0" ?>
<aiml version="2.0">
  <category>
    <pattern>WHAT IS THE MODEL NAME</pattern>
    <template>The model name is TinyTransformer.</template>
  </category>
  <category>
    <pattern>WHAT IS THE ARCHITECTURE</pattern>
    <template>This is a transformer architecture model.</template>
  </category>
  <category>
    <pattern>WHAT KIND OF MODEL IS THIS</pattern>
    <template>This is a tiny transformer model with 200 parameters.</template>
  </category>
  <category>
    <pattern>WHAT IS THE EMBEDDING DIMENSION</pattern>
    <template>The embedding dimension is 5.</template>
  </category>
  <category>
    <pattern>HOW MANY LAYERS</pattern>
    <template>The model has 1 transformer block(s).</template>
  </category>
  <category>
    <pattern>HOW MANY ATTENTION HEADS</pattern>
    <template>The model has 1 attention head(s).</template>
  </category>
  <category>
    <pattern>WHAT IS THE VOCABULARY SIZE</pattern>
    <template>The vocabulary size is 10 tokens.</template>
  </category>
  <category>
    <pattern>WHAT IS THE CONTEXT LENGTH</pattern>
    <template>The maximum context length is 5 tokens.</template>
  </category>
  <category>
    <pattern>WHAT LAYERS DOES THE MODEL HAVE</pattern>
    <template>The model has an embedding layer, self-attention layer, feed-forward network, and output layer.</template>
  </category>
  <category>
    <pattern>DESCRIBE THE EMBEDDING LAYER</pattern>
    <template>The embedding layer maps input tokens to dense vectors. It has a 10x5 weight matrix mapping 10 tokens to 5-dimensional embeddings.</template>
  </category>
  <category>
    <pattern>DESCRIBE THE ATTENTION LAYER</pattern>
    <template>The self-attention layer has query, key, and value projections with a single attention head. It computes weighted combinations of token representations.</template>
  </category>
  <category>
    <pattern>DESCRIBE THE FEEDFORWARD LAYER</pattern>
    <template>The feed-forward network consists of two linear transformations with dimension 5. It processes each position independently.</template>
  </category>
  <category>
    <pattern>DESCRIBE THE OUTPUT LAYER</pattern>
    <template>The output layer projects the 5-dimensional representations back to vocabulary size (10) to produce token predictions.</template>
  </category>
  <category>
    <pattern>HOW DOES ATTENTION WORK</pattern>
    <template>Attention computes query, key, and value matrices, then calculates attention scores by comparing queries and keys. These scores weight the values to produce the output.</template>
  </category>
  <category>
    <pattern>WHAT IS SELF ATTENTION</pattern>
    <template>Self-attention allows each position in the sequence to attend to all positions, capturing dependencies between tokens.</template>
  </category>
  <category>
    <pattern>HOW DO YOU RUN INFERENCE</pattern>
    <template>Inference flows through: token embedding, self-attention, feed-forward network, and output projection. The result is token probabilities.</template>
  </category>
  <category>
    <pattern>WHAT IS A TRANSFORMER</pattern>
    <template>A transformer is a neural network architecture based on self-attention mechanisms. It processes sequences in parallel rather than sequentially.</template>
  </category>
  <category>
    <pattern>WHAT ARE THE MODEL PARAMETERS</pattern>
    <template>The model has approximately 200 parameters split across: embedding (50), attention (75), feed-forward (50), and output (50) layers.</template>
  </category>
  <category>
    <pattern>WHAT FORMATS ARE SUPPORTED</pattern>
    <template>The model is available in GGUF, PyTorch, ONNX, JSON, TOML, AIML, OpenCog AtomSpace, and hypergraph formats.</template>
  </category>
  <category>
    <pattern>HOW CAN I USE THIS MODEL</pattern>
    <template>You can load it with the gguf-workbench Python API, use it for educational purposes, or study its structure in various representation formats.</template>
  </category>
  <category>
    <pattern>HELLO</pattern>
    <template>Hello! I am the TinyTransformer model. Ask me about my architecture, layers, or how I work.</template>
  </category>
  <category>
    <pattern>HELP</pattern>
    <template>You can ask me about: my architecture, layers, attention mechanism, parameters, or supported formats. Try 'what is the architecture' or 'describe the attention layer'.</template>
  </category>
  <category>
    <pattern>THANK YOU</pattern>
    <template>You're welcome! Let me know if you have more questions about the model.</template>
  </category>
</aiml>
